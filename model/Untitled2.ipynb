{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from builtins import *\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader():\n",
    "    def __init__(self,data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_adj(self):\n",
    "        df_adj = pd.read_csv(open(self.data_path + '/adj.tsv'), sep='\\t', dtype={0:np.int32, 1:np.int32})\n",
    "        return df_adj\n",
    "\n",
    "    def load_latest_session(self):\n",
    "        ret = []\n",
    "        for line in open(self.data_path + '/latest_sessions.txt'):\n",
    "            chunks = line.strip().split(',')\n",
    "            ret.append(chunks)\n",
    "        return ret\n",
    "\n",
    "    def load_map(self, name='user'):\n",
    "        if name == 'user':\n",
    "            file_path = self.data_path + '/user_id_map.tsv'\n",
    "        elif name == 'item':\n",
    "            file_path = self.data_path + '/item_id_map.tsv'\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        id_map = {}\n",
    "        for line in open(file_path):\n",
    "            k, v = line.strip().split('\\t')\n",
    "            id_map[k] = str(v)\n",
    "        return id_map\n",
    "\n",
    "    def load_data(self):\n",
    "        adj = self.load_adj()\n",
    "        latest_sessions = self.load_latest_session()\n",
    "        user_id_map = self.load_map('user')\n",
    "        item_id_map = self.load_map('item')\n",
    "        train = pd.read_csv(open(self.data_path + '/train.tsv'), sep='\\t', dtype={0: np.int32, 1: np.int32, 3: np.float32})\n",
    "        valid = pd.read_csv(open(self.data_path + '/valid.tsv'), sep='\\t', dtype={0: np.int32, 1: np.int32, 3: np.float32})\n",
    "        test = pd.read_csv(open(self.data_path + '/test.tsv'), sep='\\t', dtype={0: np.int32, 1: np.int32, 3: np.float32})\n",
    "        return [adj, latest_sessions, user_id_map, item_id_map, train, valid, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'H:/研究生阶段/研二上/RecommenderSystems/socialRec/data/data'\n",
    "data = Dataloader(data_path).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_info = data[0]\n",
    "latest_per_user_by_time = data[1]\n",
    "user_id_map = data[2]\n",
    "item_id_map = data[3]\n",
    "train_df = data[4]\n",
    "valid_df = data[5]\n",
    "test_df = data[6]\n",
    "data_ = [train_df,valid_df,test_df]\n",
    "all_data = pd.concat(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class construct_graph():\n",
    "    def __init__(self,datadf,num_nodes,max_degree,adj_info,latest_peruser_time):\n",
    "        self.datadf = datadf\n",
    "        self.num_nodes = num_nodes\n",
    "        self.max_degree = max_degree\n",
    "        self.adj_info = adj_info\n",
    "        self.latest_peruser_time = latest_peruser_time\n",
    "        self.visible_time = self.user_visiable_time()\n",
    "        self.adj,self.deg = self.construct_adj()\n",
    "\n",
    "    def construct_adj(self):\n",
    "        '''\n",
    "        Construct adj table used during training/validing/testing.\n",
    "        '''\n",
    "        adj = self.num_nodes*np.ones((self.num_nodes+1, self.max_degree), dtype=np.int32)\n",
    "        deg = np.zeros((self.num_nodes,))\n",
    "        missed = 0\n",
    "        for nodeid in self.datadf.UserId.unique():\n",
    "            neighbors = np.array([neighbor for neighbor in\n",
    "                             self.adj_info.loc[self.adj_info['Follower']==nodeid].Followee.unique()], dtype=np.int32)\n",
    "            deg[nodeid] = len(neighbors)\n",
    "            if len(neighbors) == 0:\n",
    "                missed += 1\n",
    "                continue\n",
    "            if len(neighbors) > self.max_degree:\n",
    "                neighbors = np.random.choice(neighbors, self.max_degree, replace=False)\n",
    "            elif len(neighbors) < self.max_degree:\n",
    "                lentmp = self.max_degree - len(neighbors)\n",
    "                neighborstmp = np.random.choice(neighbors,lentmp,replace=True)\n",
    "                neighbors = np.concatenate((neighbors,neighborstmp))\n",
    "            adj[nodeid, :] = neighbors\n",
    "        return adj, deg\n",
    "\n",
    "    def user_visiable_time(self):\n",
    "        visible_time = []\n",
    "        for l in self.latest_peruser_time:\n",
    "            timeid = max(loc for loc, val in enumerate(l) if val == 'NULL') + 1\n",
    "            visible_time.append(timeid)\n",
    "            assert timeid > 0 and timeid < len(l), 'Wrong when create visible time {}'.format(timeid)\n",
    "        return visible_time\n",
    "\n",
    "    def remove_infoless(self):\n",
    "        data = self.datadf.loc[self.deg[self.datadf['UserId']]!=0]\n",
    "        saved_session_ids = []\n",
    "        for sessid in data.SessionId.unique():\n",
    "            userid,timeid = sessid.split('_')\n",
    "            userid,timeid = int(userid),int(timeid)\n",
    "            count1 = 0\n",
    "            for neightbor in self.adj[userid, :]:\n",
    "                if self.visible_time[neightbor] <= timeid and self.deg[neightbor] > 0:\n",
    "                    count1 += 1\n",
    "\n",
    "            if count1 < self.max_degree and count1>0:\n",
    "                saved_session_ids.append(sessid)\n",
    "        return saved_session_ids\n",
    "\n",
    "    def make_layer_1(self,num_samples_1,nodeids,timeids):\n",
    "        adj_lists_1 = []\n",
    "        num_samples_1 = num_samples_1\n",
    "        nodeids = nodeids\n",
    "        timeids = timeids\n",
    "        for idx in range(len(nodeids)):\n",
    "            node = nodeids[idx]\n",
    "            timeid = timeids[idx]\n",
    "            adj = self.adj[node,:]\n",
    "            neighbors = []\n",
    "            for neighbor in adj:\n",
    "                if self.visible_time[neighbor] <= timeid and self.deg[neighbor] > 0:\n",
    "                    # for second_neighbor in self.adj[neighbor]:\n",
    "                    # \tif self.visible_time[second_neighbor] <= timeid:\n",
    "                    # \t\tfor second_neighbor in self.adj[neighbor]:\n",
    "                    # \t\t\tif self.visible_time[second_neighbor] <= timeid:\n",
    "                    # \t\t\t\tneighbors.append(neighbor)\n",
    "                    # \t\t\t\tbreak\n",
    "                    neighbors.append(neighbor)\n",
    "\n",
    "            assert len(neighbors) > 0\n",
    "            if len(neighbors) < num_samples_1:\n",
    "                neighbors = np.random.choice(neighbors, num_samples_1, replace=True)\n",
    "            elif len(neighbors) >= num_samples_1:\n",
    "                neighbors = np.random.choice(neighbors, num_samples_1, replace=False)\n",
    "            adj_lists_1.append(neighbors)\n",
    "        return np.array(adj_lists_1,dtype = np.int32)\n",
    "\n",
    "    def support_node(self):\n",
    "        self.sessionids = self.remove_infoless()\n",
    "        nodeids = [int(sessionid.split('_')[0]) for sessionid in self.sessionids]\n",
    "        timeids = [int(sessionid.split('_')[1]) for sessionid in self.sessionids]\n",
    "        self.layer1 = self.make_layer_1(num_samples_1=10,nodeids=nodeids,timeids=timeids)\n",
    "        support_layer1 = []\n",
    "        idx = 0\n",
    "        for sessionid in self.sessionids:\n",
    "            # userid = int(sessionid.split('_')[0])\n",
    "            timeid = int(sessionid.split('_')[1])\n",
    "            layer1 = self.layer1[idx]\n",
    "            idx+=1\n",
    "            per_layer = []\n",
    "            for support_node in layer1:\n",
    "                support_session_id = str(self.latest_peruser_time[support_node][timeid])\n",
    "                per_layer.append(support_session_id)\n",
    "            support_layer1.append(np.array(per_layer))\n",
    "        return np.array(support_layer1),self.sessionids,self.layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_layer1,train_sessionids,layer1 = construct_graph(datadf=train_df,num_nodes=len(user_id_map),\n",
    "                        max_degree=50,adj_info=adj_info,\n",
    "                        latest_peruser_time=latest_per_user_by_time).support_node()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155656,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessionids = np.array(train_sessionids)\n",
    "train_sessionids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sessionpadding():\n",
    "    '''\n",
    "    different sessions have different lengths, model needs same length for next thing;\n",
    "    use keras to padding items in every session.\n",
    "    datadf:dataloader.py data[4],data[5],data[6];\n",
    "    '''\n",
    "\n",
    "    def __init__(self,datadf,maxlen=20):\n",
    "        self.datadf = datadf\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def padding(self):\n",
    "        self.padding_dict = self.datadf.sort_values(by=['TimeId']).groupby('SessionId')['ItemId'].apply(list).to_dict()\n",
    "        list_v = [i for i in self.padding_dict.values()]\n",
    "        list_k = [i for i in self.padding_dict.keys()]\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for i in list_v:\n",
    "            x = i[1:]\n",
    "            y = i[:-1]\n",
    "            x_data.append(x)\n",
    "            y_data.append(y)\n",
    "        # use -1 padding values\n",
    "        x_list_v_afterpadding = tf.keras.preprocessing.sequence.pad_sequences(sequences=x_data,maxlen=self.maxlen,dtype='int32',padding='post',truncating='post',value=0).tolist()\n",
    "        y_list_v_afterpadding = tf.keras.preprocessing.sequence.pad_sequences(sequences=y_data,maxlen=self.maxlen,dtype='int32',padding='post',truncating='pre',value=0).tolist()\n",
    "        self.padding_x = dict(zip(list_k,x_list_v_afterpadding))\n",
    "        self.padding_y = dict(zip(list_k,y_list_v_afterpadding))\n",
    "        return self.padding_x,self.padding_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padding_x,train_padding_y = sessionpadding(train_df).padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sessionids have 155658 user sessionid ,trian_sessionids are array\n",
    "#train_layer1 have 155658 layer,each one has 10 friends sessionids,they are array\n",
    "#next,i should turn sessionids to padded items.\n",
    "#(nums,1)---->(nums,20)\n",
    "#(nums,10)--->(nums,10,20)\n",
    "#next step is making itemids to embedding itemids\n",
    "#(nums,20,100)\n",
    "#(nums,10,20,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155659, 20) (155659, 20) (155659, 10, 20) (155659, 10)\n"
     ]
    }
   ],
   "source": [
    "train_input_x_user_data = []\n",
    "train_input_y_user_data = []\n",
    "for sessionid in train_sessionids:\n",
    "    train_input_x_user_data.append(train_padding_x[sessionid])\n",
    "    train_input_y_user_data.append(train_padding_y[sessionid])\n",
    "train_input_x_friends_data = []\n",
    "train_input_x_friends_iddata = []\n",
    "delete_idx = []\n",
    "for nodeidx in range(len(train_layer1)):\n",
    "    node = train_layer1[nodeidx]\n",
    "    train_input_x_friends_iddata.append(node)\n",
    "    user_friend_item = []\n",
    "    idx = 0\n",
    "    for idx in range(len(node)):\n",
    "        sessionid = node[idx]\n",
    "        if sessionid not in train_padding_x.keys():\n",
    "            delete_idx.append(nodeidx)\n",
    "            for i in range(10):\n",
    "                user_friend_item.append([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "            break\n",
    "        user_friend_item.append(train_padding_x[sessionid])\n",
    "    train_input_x_friends_data.append(user_friend_item)\n",
    "    \n",
    "train_input_x_user_data = np.array(train_input_x_user_data)\n",
    "train_input_y_user_data = np.array(train_input_y_user_data)\n",
    "train_input_x_friends_data = np.array(train_input_x_friends_data)\n",
    "train_input_x_friends_iddata = np.array(train_input_x_friends_iddata)\n",
    "train_input_x_user_data = np.delete(train_input_x_user_data, delete_idx, 0)\n",
    "train_input_y_user_data = np.delete(train_input_y_user_data, delete_idx, 0)\n",
    "train_input_x_friends_data = np.delete(train_input_x_friends_data, delete_idx, 0)\n",
    "train_input_x_friends_iddata = np.delete(train_input_x_friends_iddata, delete_idx, 0)\n",
    "print(train_input_x_user_data.shape,train_input_y_user_data.shape,train_input_x_friends_data.shape,train_input_x_friends_iddata.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155659, 10) [ 2122 10593   889   889   889 10593  3311 10593  2122  3311]\n"
     ]
    }
   ],
   "source": [
    "train_input_x_friends_user_iddata = []\n",
    "for i in range(len(train_input_x_friends_iddata)):\n",
    "    sessionids = train_input_x_friends_iddata[i]\n",
    "    friendsids = []\n",
    "    for sessionid in sessionids:\n",
    "        \n",
    "        friendsid = int(sessionid.split('_')[0])\n",
    "        friendsids.append(friendsid)\n",
    "    train_input_x_friends_user_iddata.append(friendsids)\n",
    "train_input_x_friends_user_iddata = np.array(train_input_x_friends_user_iddata)\n",
    "print(train_input_x_friends_user_iddata.shape,train_input_x_friends_user_iddata[23123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155659, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_x_friends_data_0 = train_input_x_friends_data[:,0,:]\n",
    "train_input_x_friends_user_iddata_0 = train_input_x_friends_user_iddata[:,0]\n",
    "train_input_x_friends_user_iddata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_num = len(item_id_map)\n",
    "user_num = len(user_id_map)\n",
    "model = keras.Model()\n",
    "Input_user = keras.Input(shape=(20,))\n",
    "Input_user_friends = keras.Input(shape=(10,20,))\n",
    "Input_user_friends_0 = keras.Input(shape=(20,))\n",
    "Input_user_friends_0_u = keras.Input(shape=(1,))\n",
    "item_embedding_layer = keras.layers.Embedding(item_num+1,50,mask_zero=True)\n",
    "user_embedding_layer = keras.layers.Embedding(user_num+1,50)\n",
    "LSTM_friends = keras.layers.LSTM(units=100,activation='tanh',recurrent_activation='sigmoid',use_bias=True,\n",
    "                 bias_initializer='zero',\n",
    "                  return_sequences=False)\n",
    "\n",
    "user_embedding_friend_0=user_embedding_layer(Input_user_friends_0_u)\n",
    "user_embedding_friend_0 = keras.layers.Reshape((50,))(user_embedding_friend_0)\n",
    "item_embedding_friend_0=item_embedding_layer(Input_user_friends_0)\n",
    "LSTM_friend_0 = LSTM_friends(item_embedding_friend_0)\n",
    "LSTM_friend_0 = keras.layers.Dense(50, activation='relu')(LSTM_friend_0)\n",
    "friend_0 = keras.layers.concatenate([user_embedding_friend_0,LSTM_friend_0])\n",
    "\n",
    "\n",
    "item_embedding=item_embedding_layer(Input_user)\n",
    "LSTM = keras.layers.LSTM(units=100,activation='tanh',recurrent_activation='sigmoid',use_bias=True,\n",
    "                 bias_initializer='zero',\n",
    "                  return_sequences=False)(item_embedding)\n",
    "output = keras.layers.concatenate([LSTM,friend_0])\n",
    "y = keras.layers.Dense(20, activation='softmax')(output)\n",
    "model = keras.Model([Input_user,Input_user_friends_0,Input_user_friends_0_u], y)\n",
    "opt = keras.optimizers.Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\anaconda3\\envs\\tf2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1\n",
      "155600/155600 [==============================] - 981s 6ms/step - loss: 133801.48490s - loss: 133870.30 - ETA: 0s - l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2460b48a488>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_input_x_user_data,train_input_x_friends_data_0], train_input_y_user_data,\n",
    "epochs=1, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1161_329', '1161_329', '1161_329', '1161_329', '1161_329',\n",
       "       '1161_329', '1161_329', '1161_329', '1161_329', '1161_329'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_layer1[132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1447, 1277, 2793, 2794,  856, 1395, 1564, 2792,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_x_user_data[345]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 20, 50)       629600      input_19[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 50)        1325600     input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 100)          60400       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 50)           0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 50)           5050        lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 100)          60400       embedding_9[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 100)          0           reshape_2[0][0]                  \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 200)          0           lstm_8[0][0]                     \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 20)           4020        concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,085,070\n",
      "Trainable params: 2,085,070\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "input_data_item = keras.Input(shape=(1,),batch_shape=(2968,1))\n",
    "item_embedding = keras.layers.Embedding(2968, 50, input_length=1)(input_data_item)\n",
    "ids=[[112,4,5,1],[112,4,5,1],[112,4,5,1],[112,4,5,1]]\n",
    "iembs = tf.nn.embedding_lookup(item_embedding,ids)\n",
    "print(iembs)\n",
    "model = keras.models.Model(input_data_item,item_embedding)\n",
    "input_data_item=np.arange(2967)\n",
    "input_data_item = np.array(input_data_item)\n",
    "output = model_item.predict(input_data_item)\n",
    "#user_embedding\n",
    "len(data[2])\n",
    "input_data_user=np.arange(10558)\n",
    "input_data_user = np.array(input_data_user)\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(10559,50,input_length=1))\n",
    "model.compile('rmsprop','mse')\n",
    "output = model.predict(input_data_user)\n",
    "user_embedding = output\n",
    "input_data_user\n",
    "#item_embedding\n",
    "len(data[3])\n",
    "input_data_item=np.arange(2967)\n",
    "input_data_item = np.array(input_data_item)\n",
    "model_item = keras.models.Sequential()\n",
    "model_item.add(keras.layers.Embedding(2968,50,input_length=1))\n",
    "model_item.compile('rmsprop','mse')\n",
    "output = model_item.predict([1,2,3])\n",
    "item_embedding = output\n",
    "item_embedding.shape\n",
    "#dense,global\n",
    "inputs = keras.Input(shape=(1,50))\n",
    "x = keras.layers.Dense(100, activation='relu')(inputs)\n",
    "model_dense = keras.Model(inputs=inputs, outputs=x)\n",
    "output = model.predict(user_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
